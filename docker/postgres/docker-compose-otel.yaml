## This is the copy of main docker-compose.yml.
## It runs hypertrace with Postgres as document store, while default one runs with mongo
version: "2.4"
services:

  # all-in-one ingestion pipeline for hypertrace
  hypertrace-ingester:
    image: traceableai-docker.jfrog.io/hypertrace/hypertrace-ingester:test
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - DEFAULT_TENANT_ID=__default
      - SPAN_GROUPBY_SESSION_WINDOW_INTERVAL=2
      - REPLICATION_FACTOR=1
      - ENTITY_SERVICE_HOST_CONFIG=hypertrace
      - ENTITY_SERVICE_PORT_CONFIG=9001
      - ATTRIBUTE_SERVICE_HOST_CONFIG=hypertrace
      - ATTRIBUTE_SERVICE_PORT_CONFIG=9001
      - CONFIG_SERVICE_HOST_CONFIG=hypertrace
      - CONFIG_SERVICE_PORT_CONFIG=9001
      - NUM_STREAM_THREADS=1
      - PRE_CREATE_TOPICS=true
      - PRODUCER_VALUE_SERDE=org.hypertrace.core.kafkastreams.framework.serdes.GenericAvroSerde
      - JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5006
    volumes:
      - ../docker/configs/log4j2.properties:/app/resources/log4j2.properties:ro
    ports:
      - 5006:5006
    depends_on:
      kafka-zookeeper:
        condition: service_healthy

  # Third-party data services:

  # Kafka is used for streaming functionality.
  # ZooKeeper is required by Kafka and Pinot
  kafka-zookeeper:
    image: hypertrace/kafka-zookeeper:main
    networks:
      default:
        # prevents apps from having to use the hostname kafka-zookeeper
        aliases:
          - kafka
          - zookeeper
    ports:
      - 9092:9092

  # Stores spans and traces and provides aggregation functions
  pinot:
    image: hypertrace/pinot-servicemanager:main
    environment:
      - LOG_LEVEL=error
    networks:
      default:
        # Usually, Pinot is distributed, and clients connect to the controller
        aliases:
          - pinot-controller
          - pinot-server
          - pinot-broker
    ports:
      - 9000:9000
    cpu_shares: 2048
    depends_on:
      kafka-zookeeper:
        condition: service_healthy

  # new setup for metrics
  prometheus:
    container_name: prometheus
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yaml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
  # Agent
  otel-agent:
    image: otel/opentelemetry-collector-dev:latest
    command: [ "--config=/etc/otel-agent-config.yaml" ]
    volumes:
      - ./otel-agent-config.yaml:/etc/otel-agent-config.yaml
    ports:
      - "8887:8888"   # Prometheus metrics exposed by the agent
      - "14250"       # Jaeger grpc receiver
      - "14268:14268" # Jaeger http thrift receiver
      - "55678"       # OpenCensus receiver
      - "4317:4317"   # OTLP gRPC receiver
      - "9411"        # Zipkin receiver
      - "1777:1777"   # pprof extension
      - "55679:55679" # zpages extension
      - "13133"       # health_check
    depends_on:
      - otel-collector

  # Collector
  otel-collector:
    image: otel/opentelemetry-collector-dev:latest
    command: [ "--config=/etc/otel-collector-config.yaml" ]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "1888:1888"   # pprof extension
      - "8888:8888"   # Prometheus metrics exposed by the collector
      - "8889:8889"   # Prometheus exporter metrics
      - "13133:13133" # health_check extension
      - "4317"        # OTLP gRPC receiver
      - "55670:55679" # zpages extension
    depends_on:
      - jaeger-all-in-one
      - zipkin-all-in-one
      - kafka-zookeeper

  # Jaeger
  jaeger-all-in-one:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"
      - "14268"
      - "14250"

  zipkin-all-in-one:
    image: openzipkin/zipkin:latest
    ports:
      - "9411:9411"